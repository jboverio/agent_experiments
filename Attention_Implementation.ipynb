{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWdNsqagtB4jNxLOwYtL+o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jboverio/agent_experiments/blob/main/Attention_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No tokenization - strings, just numbers\n"
      ],
      "metadata": {
        "id": "ONxBs73-isrC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9g3-khCzftpm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q, K, V = self.query(x), self.key(x), self.value(x)\n",
        "        attention = torch.softmax(Q @ K.transpose(-1, -2) / (Q.size(-1)**0.5), dim=-1)\n",
        "        return attention @ V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83bb1ef5",
        "outputId": "81adc68f-b0a2-4522-95d0-e75a87b8ac9f"
      },
      "source": [
        "# Instantiate the class\n",
        "embed_dim = 64\n",
        "attention_module = SimpleAttention(embed_dim)\n",
        "\n",
        "# Create a sample input tensor\n",
        "batch_size = 1\n",
        "sequence_length = 10\n",
        "x = torch.randn(batch_size, sequence_length, embed_dim)\n",
        "\n",
        "# Pass the input through the module\n",
        "output = attention_module(x)\n",
        "\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 10, 64])\n",
            "Output shape: torch.Size([1, 10, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8669741d"
      },
      "source": [
        "# Task\n",
        "Modify the provided Python code for the `SimpleAttention` class to accept a string input instead of a numerical tensor. This involves adding tokenization and embedding steps before processing the input with the attention mechanism. Use a sample string as input and display the shape of the output tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "632c1a74"
      },
      "source": [
        "## Define vocabulary and tokenization\n",
        "\n",
        "### Subtask:\n",
        "Create a simple vocabulary and a function to tokenize the input string into a sequence of numerical indices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0bc79d3"
      },
      "source": [
        "**Reasoning**:\n",
        "The instructions require creating a vocabulary, a mapping from vocabulary to indices, and a function to tokenize a string using this mapping. These steps can be combined into a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65642586",
        "outputId": "f77bc332-0502-47ee-8523-20c6f2ce43c4"
      },
      "source": [
        "# 1. Define a list of unique characters or words that will constitute the vocabulary.\n",
        "# Using a simple character-based vocabulary for demonstration.\n",
        "vocabulary = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
        "\n",
        "# Add an 'unknown' token\n",
        "unknown_token = '<unk>'\n",
        "vocabulary.append(unknown_token)\n",
        "\n",
        "# 2. Create a dictionary to map each item in the vocabulary to a unique integer index.\n",
        "vocab_to_index = {char: i for i, char in enumerate(vocabulary)}\n",
        "\n",
        "# Define the index for the unknown token\n",
        "unknown_index = vocab_to_index[unknown_token]\n",
        "\n",
        "# 3. Write a function that takes a string as input and uses the vocabulary mapping to convert the string into a list of numerical indices.\n",
        "def tokenize_string(input_string, vocab_to_index, unknown_index):\n",
        "    \"\"\"Tokenizes an input string into a list of numerical indices.\"\"\"\n",
        "    tokenized_indices = []\n",
        "    for char in input_string.lower(): # Convert to lowercase for case-insensitivity\n",
        "        tokenized_indices.append(vocab_to_index.get(char, unknown_index))\n",
        "    return tokenized_indices\n",
        "\n",
        "# Example usage\n",
        "sample_string = \"hello world\"\n",
        "tokenized_output = tokenize_string(sample_string, vocab_to_index, unknown_index)\n",
        "print(\"Sample string:\", sample_string)\n",
        "print(\"Tokenized indices:\", tokenized_output)\n",
        "print(\"Vocabulary size:\", len(vocabulary))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample string: hello world\n",
            "Tokenized indices: [7, 4, 11, 11, 14, 26, 22, 14, 17, 11, 3]\n",
            "Vocabulary size: 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aebc3ab"
      },
      "source": [
        "## Create embedding layer\n",
        "\n",
        "### Subtask:\n",
        "Add an embedding layer to convert the numerical indices into dense vectors (embeddings) that have the `embed_dim` size expected by the `SimpleAttention` module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9786048"
      },
      "source": [
        "**Reasoning**:\n",
        "Add an embedding layer to convert the numerical indices into dense vectors using the vocabulary size and embed_dim, then tokenize a sample string and pass it through the embedding layer to demonstrate its functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33035ecb",
        "outputId": "dc4e69e9-a12f-4e9b-be44-b689449f9e72"
      },
      "source": [
        "# 2. Get the size of the vocabulary (number of unique tokens).\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "# 3. Instantiate an nn.Embedding layer.\n",
        "embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "# 4. Create a sample tensor of token indices using the tokenize_string function and the sample string \"hello world\".\n",
        "sample_string = \"hello world\"\n",
        "tokenized_indices = tokenize_string(sample_string, vocab_to_index, unknown_index)\n",
        "tokenized_tensor = torch.tensor(tokenized_indices)\n",
        "\n",
        "# 5. Pass the sample tensor of token indices through the embedding layer to get the embeddings.\n",
        "embeddings = embedding_layer(tokenized_tensor)\n",
        "\n",
        "# 6. Print the shape of the resulting embeddings.\n",
        "print(\"Shape of embeddings:\", embeddings.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embeddings: torch.Size([11, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7094c6e"
      },
      "source": [
        "## Integrate tokenization and embedding\n",
        "\n",
        "### Subtask:\n",
        "Modify the code to include the tokenization and embedding steps before passing the data to the `SimpleAttention` module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be9a8f3c"
      },
      "source": [
        "**Reasoning**:\n",
        "The goal is to modify the `SimpleAttention` class to accept a string input. This involves integrating the tokenization and embedding steps within the class's `forward` method and adding an embedding layer in the `__init__` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ac09c76",
        "outputId": "520702ed-2387-498a-ed8e-79d2edcdb5e4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, input_string, vocab_to_index, unknown_index):\n",
        "        # Tokenize the input string\n",
        "        tokenized_indices = []\n",
        "        for char in input_string.lower():\n",
        "            tokenized_indices.append(vocab_to_index.get(char, unknown_index))\n",
        "\n",
        "        # Convert tokenized indices to a tensor and add batch dimension\n",
        "        tokenized_tensor = torch.tensor(tokenized_indices).unsqueeze(0)\n",
        "\n",
        "        # Get embeddings\n",
        "        x = self.embedding(tokenized_tensor)\n",
        "\n",
        "        # Pass through attention mechanism\n",
        "        Q, K, V = self.query(x), self.key(x), self.value(x)\n",
        "        attention = torch.softmax(Q @ K.transpose(-1, -2) / (Q.size(-1)**0.5), dim=-1)\n",
        "        return attention @ V\n",
        "\n",
        "# Instantiate the modified SimpleAttention class\n",
        "vocab_size = len(vocabulary)\n",
        "embed_dim = 64\n",
        "attention_module_string_input = SimpleAttention(vocab_size, embed_dim)\n",
        "\n",
        "# Create a sample string input\n",
        "sample_string_input = \"this is a test string\"\n",
        "\n",
        "# Pass the sample string through the modified module\n",
        "output_string_input = attention_module_string_input(sample_string_input, vocab_to_index, unknown_index)\n",
        "\n",
        "# Print the output shape\n",
        "print(\"Sample string input:\", sample_string_input)\n",
        "print(\"Output shape:\", output_string_input.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample string input: this is a test string\n",
            "Output shape: torch.Size([1, 21, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "826ff213"
      },
      "source": [
        "## Prepare string input\n",
        "\n",
        "### Subtask:\n",
        "Define a sample string input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49ec42b"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a sample string input as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb56f1ce",
        "outputId": "d33048d8-1042-4c86-ec22-3e745bdc1acd"
      },
      "source": [
        "# Choose a meaningful sentence or phrase as a sample input string.\n",
        "# This string should ideally contain characters present in your defined vocabulary.\n",
        "sample_string_input = \"This is a sample sentence for testing the attention mechanism.\"\n",
        "\n",
        "# Store this chosen string in a variable named sample_string_input.\n",
        "# This was done in the previous step.\n",
        "\n",
        "# Display the sample string input to confirm.\n",
        "print(\"Sample string input:\", sample_string_input)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample string input: This is a sample sentence for testing the attention mechanism.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b1c50aa"
      },
      "source": [
        "## Process and pass input\n",
        "\n",
        "### Subtask:\n",
        "Tokenize and embed the sample string input and then pass the resulting tensor through the `SimpleAttention` module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b21520a"
      },
      "source": [
        "**Reasoning**:\n",
        "Tokenize and embed the sample string input and then pass the resulting tensor through the `SimpleAttention` module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93c1a352",
        "outputId": "63bf43d2-30fb-46b5-ad16-ccff19a92fb4"
      },
      "source": [
        "# 1. Use the tokenize_string function to convert the sample_string_input into a list of numerical indices.\n",
        "tokenized_indices = tokenize_string(sample_string_input, vocab_to_index, unknown_index)\n",
        "\n",
        "# 2. Convert the list of indices into a PyTorch tensor.\n",
        "tokenized_tensor = torch.tensor(tokenized_indices)\n",
        "\n",
        "# 3. Add a batch dimension to the tokenized tensor using unsqueeze(0).\n",
        "tokenized_tensor = tokenized_tensor.unsqueeze(0)\n",
        "\n",
        "# 4. Instantiate the SimpleAttention class with vocab_size and embed_dim.\n",
        "# This was already done in a previous step and the instance is `attention_module_string_input`.\n",
        "\n",
        "# 5. Pass the tokenized tensor through the instantiated SimpleAttention module's forward method.\n",
        "# The forward method of the modified SimpleAttention class now accepts the string directly.\n",
        "# So we call the module with the string input.\n",
        "output = attention_module_string_input(sample_string_input, vocab_to_index, unknown_index)\n",
        "\n",
        "# Display the shape of the output tensor.\n",
        "print(\"Output shape after passing through SimpleAttention module:\", output.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape after passing through SimpleAttention module: torch.Size([1, 62, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41d73862"
      },
      "source": [
        "## Display output\n",
        "\n",
        "### Subtask:\n",
        "Print the shape of the output tensor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3f7e664"
      },
      "source": [
        "**Reasoning**:\n",
        "Print the shape of the output tensor obtained from the SimpleAttention module with a descriptive label.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6f9998b",
        "outputId": "3c84f517-3503-4c35-ab4a-14a274f49480"
      },
      "source": [
        "# Print the shape of the output tensor obtained from the SimpleAttention module.\n",
        "# Use a descriptive label in the print statement, such as \"Shape of the output tensor:\".\n",
        "print(\"Shape of the output tensor:\", output.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the output tensor: torch.Size([1, 62, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c50eb78"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A character-based vocabulary was successfully created, including lowercase English alphabet characters, a space, and an unknown token.\n",
        "*   A function `tokenize_string` was implemented to convert an input string into a list of numerical indices based on the defined vocabulary.\n",
        "*   An `nn.Embedding` layer was successfully created and used to convert the tokenized numerical indices into dense vectors (embeddings) of size 64. The embedding process transformed a tensor of token indices with shape `torch.Size([11])` into an embedding tensor with shape `torch.Size([11, 64])` for the sample string \"hello world\".\n",
        "*   The `SimpleAttention` class was modified to accept a string input directly. The tokenization and embedding steps were integrated within the class's `forward` method.\n",
        "*   Processing the sample string \"this is a test string\" through the modified `SimpleAttention` module resulted in an output tensor with the shape `torch.Size([1, 21, 64])`.\n",
        "*   Processing the sample string \"This is a sample sentence for testing the attention mechanism.\" through the modified `SimpleAttention` module resulted in an output tensor with the shape `torch.Size([1, 62, 64])`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current implementation processes strings character by character. For more complex natural language processing tasks, consider using word-based tokenization or sub-word tokenization methods (e.g., WordPiece, BPE) and a larger vocabulary.\n",
        "*   The attention mechanism currently calculates attention over the entire input sequence. For longer sequences, consider implementing masked attention or other attention variants to improve efficiency and potentially performance.\n"
      ]
    }
  ]
}